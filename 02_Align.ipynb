{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to align files using DTW or a dropout-augmented DTW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as lb\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align with basic DTW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains a cython implementation of basic DTW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "cimport cython\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "DTYPE_INT32 = np.int32\n",
    "ctypedef np.int32_t DTYPE_INT32_t\n",
    "\n",
    "DTYPE_FLOAT = np.float64\n",
    "ctypedef np.float64_t DTYPE_FLOAT_t\n",
    "\n",
    "cdef DTYPE_FLOAT_t MAX_FLOAT = float('inf')\n",
    "\n",
    "# careful, without bounds checking can mess up memory - also can't use negative indices I think (like x[-1])\n",
    "@cython.boundscheck(False) # turn off bounds-checking for entire function\n",
    "def DTW_Cost_To_AccumCostAndSteps(Cin, parameter):\n",
    "    '''\n",
    "    Inputs\n",
    "        C: The cost Matrix\n",
    "    '''\n",
    "\n",
    "\n",
    "    '''\n",
    "    Section for checking and catching errors in the inputs\n",
    "    '''\n",
    "\n",
    "    cdef np.ndarray[DTYPE_FLOAT_t, ndim=2] C\n",
    "    try:\n",
    "        C = np.array(Cin, dtype=DTYPE_FLOAT)\n",
    "    except TypeError:\n",
    "        print(bcolors.FAIL + \"FAILURE: The type of the cost matrix is wrong - please pass in a 2-d numpy array\" + bcolors.ENDC)\n",
    "        return [-1, -1, -1]\n",
    "    except ValueError:\n",
    "        print(bcolors.FAIL + \"FAILURE: The type of the elements in the cost matrix is wrong - please have each element be a float (perhaps you passed in a matrix of ints?)\" + bcolors.ENDC)\n",
    "        return [-1, -1, -1]\n",
    "\n",
    "    cdef np.ndarray[np.uint32_t, ndim=1] dn\n",
    "    cdef np.ndarray[np.uint32_t, ndim=1] dm\n",
    "    cdef np.ndarray[DTYPE_FLOAT_t, ndim=1] dw\n",
    "    # make sure dn, dm, and dw are setup\n",
    "    # dn loading and exception handling\n",
    "    if ('dn'  in parameter.keys()):\n",
    "        try:\n",
    "\n",
    "            dn = np.array(parameter['dn'], dtype=np.uint32)\n",
    "        except TypeError:\n",
    "            print(bcolors.FAIL + \"FAILURE: The type of dn (row steps) is wrong - please pass in a 1-d numpy array that holds uint32s\" + bcolors.ENDC)\n",
    "            return [-1, -1, -1]\n",
    "        except ValueError:\n",
    "            print(bcolors.FAIL + \"The type of the elements in dn (row steps) is wrong - please have each element be a uint32 (perhaps you passed a long?). You can specify this when making a numpy array like: np.array([1,2,3],dtype=np.uint32)\" + bcolors.ENDC)\n",
    "            return [-1, -1, -1]\n",
    "    else:\n",
    "        dn = np.array([1, 1, 0], dtype=np.uint32)\n",
    "    # dm loading and exception handling\n",
    "    if 'dm'  in parameter.keys():\n",
    "        try:\n",
    "            dm = np.array(parameter['dm'], dtype=np.uint32)\n",
    "        except TypeError:\n",
    "            print(bcolors.FAIL + \"FAILURE: The type of dm (col steps) is wrong - please pass in a 1-d numpy array that holds uint32s\" + bcolors.ENDC)\n",
    "            return [-1, -1, -1]\n",
    "        except ValueError:\n",
    "            print(bcolors.FAIL + \"FAILURE: The type of the elements in dm (col steps) is wrong - please have each element be a uint32 (perhaps you passed a long?). You can specify this when making a numpy array like: np.array([1,2,3],dtype=np.uint32)\" + bcolors.ENDC)\n",
    "            return [-1, -1, -1]\n",
    "    else:\n",
    "        print(bcolors.FAIL + \"dm (col steps) was not passed in (gave default value [1,0,1]) \" + bcolors.ENDC)\n",
    "        dm = np.array([1, 0, 1], dtype=np.uint32)\n",
    "    # dw loading and exception handling\n",
    "    if 'dw'  in parameter.keys():\n",
    "        try:\n",
    "            dw = np.array(parameter['dw'], dtype=DTYPE_FLOAT)\n",
    "        except TypeError:\n",
    "            print(bcolors.FAIL + \"FAILURE: The type of dw (step weights) is wrong - please pass in a 1-d numpy array that holds floats\" + bcolors.ENDC)\n",
    "            return [-1, -1, -1]\n",
    "        except ValueError:\n",
    "            print(bcolors.FAIL + \"FAILURE:The type of the elements in dw (step weights) is wrong - please have each element be a float (perhaps you passed ints or a long?). You can specify this when making a numpy array like: np.array([1,2,3],dtype=np.float64)\" + bcolors.ENDC)\n",
    "            return [-1, -1, -1]\n",
    "    else:\n",
    "        dw = np.array([1, 1, 1], dtype=DTYPE_FLOAT)\n",
    "        print(bcolors.FAIL + \"dw (step weights) was not passed in (gave default value [1,1,1]) \" + bcolors.ENDC)\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Section where types are given to the variables we're going to use \n",
    "    '''\n",
    "    # create matrices to store our results (D and E)\n",
    "    cdef DTYPE_INT32_t numRows = C.shape[0] # only works with np arrays, use np.shape(x) will work on lists? want to force to use np though?\n",
    "    cdef DTYPE_INT32_t numCols = C.shape[1]\n",
    "    cdef DTYPE_INT32_t numDifSteps = np.size(dw)\n",
    "\n",
    "    cdef unsigned int maxRowStep = max(dn)\n",
    "    cdef unsigned int maxColStep = max(dm)\n",
    "\n",
    "    cdef np.ndarray[np.uint32_t, ndim=2] steps = np.zeros((numRows,numCols), dtype=np.uint32)\n",
    "    cdef np.ndarray[DTYPE_FLOAT_t, ndim=2] accumCost = np.ones((maxRowStep + numRows, maxColStep + numCols), dtype=DTYPE_FLOAT) * MAX_FLOAT\n",
    "\n",
    "    cdef DTYPE_FLOAT_t bestCost\n",
    "    cdef DTYPE_INT32_t bestCostIndex\n",
    "    cdef DTYPE_FLOAT_t costForStep\n",
    "    cdef unsigned int row, col\n",
    "    cdef unsigned int stepIndex\n",
    "\n",
    "    '''\n",
    "    The start of the actual algorithm, now that all our variables are set up\n",
    "    '''\n",
    "    # initializing the cost matrix - depends on whether its subsequence DTW\n",
    "    # essentially allow us to hop on the bottom anywhere (so could start partway through one of the signals)\n",
    "    if parameter['SubSequence']:\n",
    "        for col in range(numCols):\n",
    "            accumCost[maxRowStep, col + maxColStep] = C[0, col]\n",
    "    else:\n",
    "        accumCost[maxRowStep, maxColStep] = C[0,0]\n",
    "\n",
    "    # filling the accumulated cost matrix\n",
    "    for row in range(maxRowStep, numRows + maxRowStep, 1):\n",
    "        for col in range(maxColStep, numCols + maxColStep, 1):\n",
    "            bestCost = accumCost[<unsigned int>row, <unsigned int>col] # initialize with what's there - so if is an entry point, then can start low\n",
    "            bestCostIndex = 0\n",
    "            # go through each step, find the best one\n",
    "            for stepIndex in range(numDifSteps):\n",
    "                #costForStep = accumCost[<unsigned int>(row - dn[<unsigned int>(stepIndex)]), <unsigned int>(col - dm[<unsigned int>(stepIndex)])] + dw[<unsigned int>(stepIndex)] * C[<unsigned int>(row - maxRowStep), <unsigned int>(col - maxColStep)]\n",
    "                costForStep = accumCost[<unsigned int>((row - dn[(stepIndex)])), <unsigned int>((col - dm[(stepIndex)]))] + dw[stepIndex] * C[<unsigned int>(row - maxRowStep), <unsigned int>(col - maxColStep)]\n",
    "                if costForStep < bestCost:\n",
    "                    bestCost = costForStep\n",
    "                    bestCostIndex = stepIndex\n",
    "            # save the best cost and best cost index\n",
    "            accumCost[row, col] = bestCost\n",
    "            steps[<unsigned int>(row - maxRowStep), <unsigned int>(col - maxColStep)] = bestCostIndex\n",
    "\n",
    "    # return the accumulated cost along with the matrix of steps taken to achieve that cost\n",
    "    return [accumCost[maxRowStep:, maxColStep:], steps]\n",
    "\n",
    "@cython.boundscheck(False) # turn off bounds-checking for entire function\n",
    "def DTW_GetPath(np.ndarray[DTYPE_FLOAT_t, ndim=2] accumCost, np.ndarray[np.uint32_t, ndim=2] stepsForCost, parameter):\n",
    "    '''\n",
    "\n",
    "    Parameter should have: 'dn', 'dm', 'dw', 'SubSequence'\n",
    "    '''\n",
    "\n",
    "    cdef np.ndarray[unsigned int, ndim=1] dn\n",
    "    cdef np.ndarray[unsigned int, ndim=1] dm\n",
    "    cdef np.uint8_t subseq\n",
    "    cdef np.int32_t startCol # added\n",
    "    # make sure dn, dm, and dw are setup\n",
    "    if ('dn'  in parameter.keys()):\n",
    "        dn = parameter['dn']\n",
    "    else:\n",
    "        dn = np.array([1, 1, 0], dtype=DTYPE_INT32)\n",
    "    if 'dm'  in parameter.keys():\n",
    "        dm = parameter['dm']\n",
    "    else:\n",
    "        dm = np.array([1, 0, 1], dtype=DTYPE_INT32)\n",
    "    if 'SubSequence' in parameter.keys():\n",
    "        subseq = parameter['SubSequence']\n",
    "    else:\n",
    "        subseq = 0\n",
    "    \n",
    "    # added START\n",
    "    if 'startCol' in parameter.keys(): \n",
    "        startCol = parameter['startCol']\n",
    "    else:\n",
    "        startCol = -1\n",
    "    # added END\n",
    "\n",
    "    cdef np.uint32_t numRows\n",
    "    cdef np.uint32_t numCols\n",
    "    cdef np.uint32_t curRow\n",
    "    cdef np.uint32_t curCol\n",
    "    cdef np.uint32_t endCol\n",
    "    cdef DTYPE_FLOAT_t endCost\n",
    "\n",
    "    numRows = accumCost.shape[0]\n",
    "    numCols = accumCost.shape[1]\n",
    "\n",
    "    # either start at the far corner (non sub-sequence)\n",
    "    # or start at the lowest cost entry in the last row (sub-sequence)\n",
    "    # where all of the signal along the row has been used, but only a \n",
    "    # sub-sequence of the signal along the columns has to be used\n",
    "    curRow = numRows - 1\n",
    "    if subseq:\n",
    "        curCol = np.argmin(accumCost[numRows - 1, :])\n",
    "    else:\n",
    "        curCol = numCols - 1\n",
    "        \n",
    "    # added - if specified, overrides above\n",
    "    if startCol >= 0:\n",
    "        curCol = startCol\n",
    "\n",
    "    endCol = curCol\n",
    "    endCost = accumCost[curRow, curCol]\n",
    "\n",
    "    cdef np.uint32_t curRowStep\n",
    "    cdef np.uint32_t curColStep\n",
    "    cdef np.uint32_t curStepIndex\n",
    "\n",
    "\n",
    "    cdef np.ndarray[np.uint32_t, ndim=2] path = np.zeros((2, numRows + numCols), dtype=np.uint32) # make as large as could need, then chop at the end\n",
    "    path[0, 0] = curRow\n",
    "    path[1, 0] = curCol\n",
    "\n",
    "    cdef np.uint32_t stepsInPath = 1 # starts at one, we add in one before looping\n",
    "    cdef np.uint32_t stepIndex = 0\n",
    "    cdef np.int8_t done = (subseq and curRow == 0) or (curRow == 0 and curCol == 0)\n",
    "    while not done:\n",
    "        if accumCost[curRow, curCol] == MAX_FLOAT:\n",
    "            print('A path is not possible')\n",
    "            break\n",
    "\n",
    "        # you're done if you've made it to the bottom left (non sub-sequence)\n",
    "        # or just the bottom (sub-sequence)\n",
    "        # find the step size\n",
    "        curStepIndex = stepsForCost[curRow, curCol]\n",
    "        curRowStep = dn[curStepIndex]\n",
    "        curColStep = dm[curStepIndex]\n",
    "        # backtrack by 1 step\n",
    "        curRow = curRow - curRowStep\n",
    "        curCol = curCol - curColStep\n",
    "        # add your new location onto the path\n",
    "        path[0, stepsInPath] = curRow\n",
    "        path[1, stepsInPath] = curCol\n",
    "        stepsInPath = stepsInPath + 1\n",
    "        # check to see if you're done\n",
    "        done = (subseq and curRow == 0) or (curRow == 0 and curCol == 0)\n",
    "\n",
    "    # reverse the path (a matrix with two rows) and return it\n",
    "    return [np.fliplr(path[:, 0:stepsInPath]), endCol, endCost]\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignDTW(featfile1, featfile2, steps, weights, downsample, outfile = None, C = None):\n",
    "    \n",
    "    if C is None:\n",
    "        F1 = np.load(featfile1) # 12 x N\n",
    "        F2 = np.load(featfile2) # 12 x M\n",
    "        if max(F1.shape[1], F2.shape[1]) / min(F1.shape[1], F2.shape[1]) >= 2: # no valid path possible\n",
    "            if outfile:\n",
    "                pickle.dump(None, open(outfile, 'wb'))\n",
    "            return None\n",
    "        C = 1 - F1[:,0::downsample].T @ F2[:,0::downsample] # cos distance metric\n",
    "\n",
    "    dn = steps[:,0].astype(np.uint32)\n",
    "    dm = steps[:,1].astype(np.uint32)\n",
    "    parameters = {'dn': dn, 'dm': dm, 'dw': weights, 'SubSequence': False}\n",
    "    [D, s] = DTW_Cost_To_AccumCostAndSteps(C, parameters)\n",
    "    [wp, endCol, endCost] = DTW_GetPath(D, s, parameters)\n",
    "    if outfile:\n",
    "        pickle.dump(wp, open(outfile, 'wb'))\n",
    "    return wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignDTW_batch(querylist, featdir1, featdir2, outdir, n_cores, steps, weights, downsample):\n",
    "    \n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # prep inputs for parallelization\n",
    "    inputs = []\n",
    "    with open(querylist, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(' ')\n",
    "            assert len(parts) == 2\n",
    "            featfile1 = (featdir1 / parts[0]).with_suffix('.npy')\n",
    "            featfile2 = (featdir2 / parts[1]).with_suffix('.npy')\n",
    "            queryid = os.path.basename(parts[0]) + '__' + os.path.basename(parts[1])\n",
    "            outfile = (outdir / queryid).with_suffix('.pkl')\n",
    "            if os.path.exists(outfile):\n",
    "                print(f\"Skipping {outfile}\")\n",
    "            else:\n",
    "                inputs.append((featfile1, featfile2, steps, weights, downsample, outfile))\n",
    "\n",
    "    # process files in parallel\n",
    "    pool = multiprocessing.Pool(processes = n_cores)\n",
    "    pool.starmap(alignDTW, inputs)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featfile1 = 'features/clean/Chopin_Op068No3/Chopin_Op068No3_Tomsic-1995_pid9190-11.npy'\n",
    "# featfile2 = 'features/clean/Chopin_Op068No3/Chopin_Op068No3_Cortot-1951_pid9066b-19.npy'\n",
    "# steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "# weights = np.array([2,3,3])\n",
    "# downsample = 1\n",
    "# wp = alignDTW(featfile1, featfile2, steps, weights, downsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_list = 'cfg_files/query.test.list'\n",
    "# featdir1 = Path('features/clean')\n",
    "# featdir2 = Path('features/clean')\n",
    "# outdir = Path('experiments_test/dtw_clean')\n",
    "# n_cores = 1\n",
    "# steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "# weights = np.array([2,3,3])\n",
    "# downsample = 1\n",
    "# inputs = alignDTW_batch(query_list, featdir1, featdir2, outdir, n_cores, steps, weights, downsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align with Segmental DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignSegmentalDTW(featfile1, featfile2, steps, weights, downsample, numSegments, outfile = None, C = None):\n",
    "    \n",
    "    # compute cost matrix\n",
    "    if C is None:\n",
    "        F1 = np.load(featfile1) # 12 x N\n",
    "        F2 = np.load(featfile2) # 12 x M\n",
    "        if max(F1.shape[1], F2.shape[1]) / min(F1.shape[1], F2.shape[1]) >= 2: # no valid path possible\n",
    "            if outfile:\n",
    "                pickle.dump(None, open(outfile, 'wb'))\n",
    "            return None\n",
    "        C = 1 - F1[:,0::downsample].T @ F2[:,0::downsample] # cos distance metric\n",
    "\n",
    "    # run subseqDTW on chunks\n",
    "    seglen = int(np.ceil(C.shape[0] / numSegments))\n",
    "    dn1 = steps[:,0].astype(np.uint32)\n",
    "    dm1 = steps[:,1].astype(np.uint32)\n",
    "    dw1 = weights\n",
    "    params1 = {'dn': dn1, 'dm': dm1, 'dw': dw1, 'SubSequence': True}\n",
    "    Dparts = []\n",
    "    Bparts = []\n",
    "    for i in range(numSegments):\n",
    "        Cpart = C[i*seglen : min((i+1)*seglen, C.shape[0]), :]\n",
    "        [D, B] = DTW_Cost_To_AccumCostAndSteps(Cpart, params1)\n",
    "        Dparts.append(D)\n",
    "        Bparts.append(B)\n",
    "\n",
    "    # run segment-level DP\n",
    "    Cseg = np.zeros((numSegments+1, C.shape[1]))\n",
    "    for i in range(len(Dparts)):\n",
    "        Cseg[i+1,:] = Dparts[i][-1,:]\n",
    "    dn2 = np.array([0, 1], dtype=np.uint32)\n",
    "    dm2 = np.array([1, seglen//np.max(steps[:,0])], dtype=np.uint32)\n",
    "    dw2 = np.array([0, 1])\n",
    "    params2 = {'dn': dn2, 'dm': dm2, 'dw': dw2, 'SubSequence': False}\n",
    "    [Dseg, Bseg] = DTW_Cost_To_AccumCostAndSteps(Cseg, params2)\n",
    "    [wpseg, _, _] = DTW_GetPath(Dseg, Bseg, params2)\n",
    "\n",
    "    # backtrace\n",
    "    segmentEndIdxs = getSegmentEndingLocs(wpseg)\n",
    "    wps = []\n",
    "    for i, endidx in enumerate(segmentEndIdxs):\n",
    "        params3 = {'dn': dn1, 'dm': dm1, 'dw': dw1, 'SubSequence': True, 'startCol': endidx}\n",
    "        [wpchunk, _, _] = DTW_GetPath(Dparts[i], Bparts[i], params3)\n",
    "        wpchunk[0,:] = wpchunk[0,:] + i*seglen  # account for relative offset\n",
    "        wps.append(wpchunk.copy())\n",
    "    wp_merged = np.hstack(wps)\n",
    "\n",
    "    if outfile:\n",
    "        pickle.dump(wp_merged, open(outfile, 'wb'))\n",
    "\n",
    "    return wp_merged#, segmentEndIdxs, seglen, Dparts, Bparts, params1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignSegmentalDTW_parallel(featfile1, featfile2, steps, weights, downsample, numSegments, outfile = None, C = None):\n",
    "    '''\n",
    "    Parallelized version of alignSegmentalDTW\n",
    "    '''\n",
    "    \n",
    "    # compute cost matrix\n",
    "    if C is None:\n",
    "        F1 = np.load(featfile1) # 12 x N\n",
    "        F2 = np.load(featfile2) # 12 x M\n",
    "        if max(F1.shape[1], F2.shape[1]) / min(F1.shape[1], F2.shape[1]) >= 2: # no valid path possible\n",
    "            if outfile:\n",
    "                pickle.dump(None, open(outfile, 'wb'))\n",
    "            return None\n",
    "        C = 1 - F1[:,0::downsample].T @ F2[:,0::downsample] # cos distance metric\n",
    "    \n",
    "    # prep data for parallelization\n",
    "    seglen = int(np.ceil(C.shape[0] / numSegments))\n",
    "    dn1 = steps[:,0].astype(np.uint32)\n",
    "    dm1 = steps[:,1].astype(np.uint32)\n",
    "    dw1 = weights\n",
    "    params1 = {'dn': dn1, 'dm': dm1, 'dw': dw1, 'SubSequence': True}\n",
    "    inputs = []\n",
    "    for i in range(numSegments):\n",
    "        Cpart = C[i*seglen : min((i+1)*seglen, C.shape[0]), :]\n",
    "        inputs.append((Cpart, params1))\n",
    "\n",
    "    # run subseqDTW on chunks (parallelized)\n",
    "    pool = multiprocessing.Pool(numSegments)\n",
    "    outputs = pool.starmap(DTW_Cost_To_AccumCostAndSteps, inputs) # list of (D,B) tuples\n",
    "    pool.close() \n",
    "        \n",
    "    # run segment-level DP\n",
    "    Cseg = np.zeros((numSegments+1, C.shape[1]))\n",
    "    for i, (Dpart, Bpart) in enumerate(outputs):\n",
    "        Cseg[i+1,:] = Dpart[-1,:]\n",
    "    dn2 = np.array([0, 1], dtype=np.uint32)\n",
    "    dm2 = np.array([1, seglen//np.max(steps[:,0])], dtype=np.uint32)\n",
    "    dw2 = np.array([0, 1])\n",
    "    params2 = {'dn': dn2, 'dm': dm2, 'dw': dw2, 'SubSequence': False}\n",
    "    [Dseg, Bseg] = DTW_Cost_To_AccumCostAndSteps(Cseg, params2)\n",
    "    [wpseg, _, _] = DTW_GetPath(Dseg, Bseg, params2)\n",
    "    \n",
    "    # backtrace\n",
    "    segmentEndIdxs = getSegmentEndingLocs(wpseg)\n",
    "    wps = []\n",
    "    for i, endidx in enumerate(segmentEndIdxs):\n",
    "        params3 = {'dn': dn1, 'dm': dm1, 'dw': dw1, 'SubSequence': True, 'startCol': endidx}\n",
    "        Dpart, Bpart = outputs[i]\n",
    "        [wpchunk, _, _] = DTW_GetPath(Dpart, Bpart, params3)\n",
    "        wpchunk[0,:] = wpchunk[0,:] + i*seglen  # account for relative offset\n",
    "        wps.append(wpchunk.copy())\n",
    "    wp_merged = np.hstack(wps)\n",
    "    \n",
    "    if outfile:\n",
    "        pickle.dump(wp_merged, open(outfile, 'wb'))\n",
    "        \n",
    "    return wp_merged#, segmentEndIdxs, seglen, Dparts, Bparts, params1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSegmentEndingLocs(wp):\n",
    "    prevLoc = wp[:,0] # [r,c]\n",
    "    endLocs = []\n",
    "    for i in range(wp.shape[1]):\n",
    "        curLoc = wp[:,i]\n",
    "        if curLoc[0] != prevLoc[0]: # if row changes\n",
    "            endLocs.append(curLoc[1])\n",
    "        prevLoc = curLoc\n",
    "        \n",
    "    return endLocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featfile1 = 'features/clean/Chopin_Op017No4/Chopin_Op017No4_Afanassiev-2001_pid9130-01.npy'\n",
    "# featfile2 = 'features/clean/Chopin_Op017No4/Chopin_Op017No4_Ben-Or-1989_pid9161-12.npy'\n",
    "# #featfile1 = 'features/clean/Chopin_Op017No4/Chopin_Op017No4_Ashkenazy-1981_pid9058-13.npy'\n",
    "# #featfile2 = 'features/clean/Chopin_Op017No4/Chopin_Op017No4_Beliavsky-2004_pid9152-13.npy'\n",
    "# #featfile2 = 'features/clean/Chopin_Op017No4/Chopin_Op017No4_Biret-1990_pid9062-13.npy'\n",
    "# #featfile1 = 'features/clean/Chopin_Op068No3/Chopin_Op068No3_Jonas-1947_pid9096-03.npy'\n",
    "# #featfile2 = 'features/clean/Chopin_Op068No3/Chopin_Op068No3_Richter-1976_pid9172-12.npy'\n",
    "# steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "# weights = np.array([1,1,2])\n",
    "# downsample = 1\n",
    "# numSegments = 5\n",
    "# wp = alignSegmentalDTW(featfile1, featfile2, steps, weights, downsample, numSegments)\n",
    "# wp2 = alignSegmentalDTW_parallel(featfile1, featfile2, steps, weights, downsample, numSegments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignSegmentalDTW_batch(querylist, featdir1, featdir2, outdir, n_cores, steps, weights, downsample, numSegments, fn):\n",
    "\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # prep inputs for parallelization\n",
    "    inputs = []\n",
    "    with open(querylist, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(' ')\n",
    "            assert len(parts) == 2\n",
    "            featfile1 = (featdir1 / parts[0]).with_suffix('.npy')\n",
    "            featfile2 = (featdir2 / parts[1]).with_suffix('.npy')\n",
    "            queryid = os.path.basename(parts[0]) + '__' + os.path.basename(parts[1])\n",
    "            outfile = (outdir / queryid).with_suffix('.pkl')\n",
    "            if os.path.exists(outfile):\n",
    "                print(f\"Skipping {outfile}\")\n",
    "            else:\n",
    "                inputs.append((featfile1, featfile2, steps, weights, downsample, numSegments, outfile))\n",
    "\n",
    "    # process files in parallel\n",
    "    pool = multiprocessing.Pool(processes = n_cores)\n",
    "    pool.starmap(fn, inputs)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_list = 'cfg_files/query.test.list'\n",
    "# featdir1 = Path('features/clean')\n",
    "# featdir2 = Path('features/clean')\n",
    "# n_cores = 1\n",
    "# steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "# weights = np.array([1,1,2])\n",
    "# downsample = 1\n",
    "# #segmentVals = [2, 4, 8, 16, 32] \n",
    "# segmentVals = [32]\n",
    "# for numSegments in segmentVals:\n",
    "#     outdir = Path(f'experiments_test/segmental_{numSegments}_clean')\n",
    "#     alignSegmentalDTW_batch(query_list, featdir1, featdir2, outdir, n_cores, steps, weights, downsample, numSegments, alignSegmentalDTW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align with Parallelized Approximate DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "cimport cython\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "DTYPE_INT32 = np.int32\n",
    "ctypedef np.int32_t DTYPE_INT32_t\n",
    "\n",
    "DTYPE_FLOAT = np.float64\n",
    "ctypedef np.float64_t DTYPE_FLOAT_t\n",
    "\n",
    "cdef DTYPE_FLOAT_t MAX_FLOAT = float('inf')\n",
    "\n",
    "# careful, without bounds checking can mess up memory - also can't use negative indices I think (like x[-1])\n",
    "@cython.boundscheck(False) # turn off bounds-checking for entire function\n",
    "def Segment_DP(np.ndarray[DTYPE_FLOAT_t, ndim=2] C, np.ndarray[np.int32_t, ndim=2] T):\n",
    "\n",
    "    cdef DTYPE_INT32_t numRows = C.shape[0]\n",
    "    cdef DTYPE_INT32_t numCols = C.shape[1]    \n",
    "    cdef np.ndarray[np.int32_t, ndim=2] steps = np.zeros((numRows+1,numCols), dtype=np.int32)\n",
    "    cdef np.ndarray[DTYPE_FLOAT_t, ndim=2] accumCost = np.ones((numRows+1, numCols), dtype=DTYPE_FLOAT) * MAX_FLOAT\n",
    "\n",
    "    cdef unsigned int row, col\n",
    "    cdef DTYPE_FLOAT_t skipCost\n",
    "    cdef np.int32_t jumpStartCol\n",
    "    cdef DTYPE_FLOAT_t jumpCost\n",
    "\n",
    "    # initialize\n",
    "    for row in range(numRows+1):\n",
    "        for col in range(numCols):\n",
    "            steps[row, col] = -1 # skip by default\n",
    "    for col in range(numCols):\n",
    "        accumCost[0, col] = 0 # all inf except first row\n",
    "        \n",
    "    # dynamic programming\n",
    "    for row in range(1, numRows+1):\n",
    "        for col in range(numCols):\n",
    "            \n",
    "            # skip transition\n",
    "            if col == 0:\n",
    "                skipCost = MAX_FLOAT\n",
    "            else:\n",
    "                skipCost = accumCost[row, col-1]\n",
    "            accumCost[row, col] = skipCost\n",
    "            # best step is skip by default, so don't need to assign\n",
    "            \n",
    "            # jump transition\n",
    "            jumpStartCol = T[row-1, col]\n",
    "            if jumpStartCol >= 0: # valid subsequence path\n",
    "                jumpCost = accumCost[row-1, jumpStartCol] + C[row-1, col]\n",
    "                if jumpCost < skipCost:\n",
    "                    accumCost[row, col] = jumpCost\n",
    "                    steps[row, col] = jumpStartCol\n",
    "\n",
    "    return [accumCost, steps]\n",
    "\n",
    "@cython.boundscheck(False) # turn off bounds-checking for entire function\n",
    "def Segment_Backtrace(np.ndarray[DTYPE_FLOAT_t, ndim=2] accumCost, np.ndarray[np.int32_t, ndim=2] steps):\n",
    "\n",
    "    cdef np.uint32_t numRows = accumCost.shape[0]\n",
    "    cdef np.uint32_t numCols = accumCost.shape[1]\n",
    "    cdef np.uint32_t curRow = numRows - 1\n",
    "    cdef np.uint32_t curCol = numCols - 1\n",
    "    cdef np.int32_t jump\n",
    "    cdef np.ndarray[np.uint32_t, ndim=1] path = np.zeros(numRows-1, dtype=np.uint32)\n",
    "    cdef np.uint32_t pathElems = 0\n",
    "\n",
    "    while curRow > 0:\n",
    "        if accumCost[curRow, curCol] == MAX_FLOAT:\n",
    "            print('A path is not possible')\n",
    "            break\n",
    "\n",
    "        jump = steps[curRow, curCol]\n",
    "        if jump < 0: # skip\n",
    "            curCol = curCol - 1\n",
    "        else: # jump\n",
    "            path[pathElems] = curCol\n",
    "            pathElems = pathElems + 1\n",
    "            curRow = curRow - 1\n",
    "            curCol = jump\n",
    "\n",
    "    return path[::-1]\n",
    "\n",
    "@cython.boundscheck(False) # turn off bounds-checking for entire function\n",
    "def calc_Tseg(np.ndarray[DTYPE_FLOAT_t, ndim=2] accumCost, np.ndarray[np.uint32_t, ndim=2] stepsForCost, parameter):\n",
    "    '''\n",
    "\n",
    "    Parameter should have: 'dn', 'dm'\n",
    "    '''\n",
    "\n",
    "    cdef np.ndarray[unsigned int, ndim=1] dn\n",
    "    cdef np.ndarray[unsigned int, ndim=1] dm\n",
    "    cdef np.uint32_t numRows = accumCost.shape[0]\n",
    "    cdef np.uint32_t numCols = accumCost.shape[1]\n",
    "    cdef np.ndarray[np.int32_t, ndim=1] startLocs = np.zeros(numCols, dtype=np.int32)\n",
    "    cdef np.uint32_t endCol\n",
    "    cdef np.uint32_t curRow\n",
    "    cdef np.uint32_t curCol\n",
    "    cdef np.uint32_t curStepIndex\n",
    "\n",
    "    # get step transitions\n",
    "    if ('dn'  in parameter.keys()):\n",
    "        dn = parameter['dn']\n",
    "    else:\n",
    "        dn = np.array([1, 1, 0], dtype=DTYPE_INT32)\n",
    "    if 'dm'  in parameter.keys():\n",
    "        dm = parameter['dm']\n",
    "    else:\n",
    "        dm = np.array([1, 0, 1], dtype=DTYPE_INT32)\n",
    "\n",
    "    # backtrace from every location\n",
    "    for endCol in range(numCols):\n",
    "        curCol = endCol\n",
    "        curRow = numRows - 1\n",
    "        while curRow > 0:\n",
    "            if accumCost[curRow, curCol] == MAX_FLOAT: # no valid path\n",
    "                startLocs[curCol] = -1\n",
    "                break\n",
    "\n",
    "            curStepIndex = stepsForCost[curRow, curCol]\n",
    "            curRow = curRow - dn[curStepIndex]\n",
    "            curCol = curCol - dm[curStepIndex]\n",
    "            if curRow == 0:\n",
    "                startLocs[endCol] = curCol\n",
    "                \n",
    "    return startLocs\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignParDTW(featfile1, featfile2, steps, weights, downsample, numSegments, outfile = None):\n",
    "    \n",
    "    # compute cost matrix\n",
    "    F1 = np.load(featfile1) # 12 x N\n",
    "    F2 = np.load(featfile2) # 12 x M\n",
    "    swap = (F1.shape[1] > F2.shape[1])\n",
    "    if swap:\n",
    "        F1, F2 = F2, F1 # make the shorter sequence the query\n",
    "    if max(F1.shape[1], F2.shape[1]) / min(F1.shape[1], F2.shape[1]) >= 2: # no valid path possible\n",
    "        if outfile:\n",
    "            pickle.dump(None, open(outfile, 'wb'))\n",
    "        return None\n",
    "    C = 1 - F1[:,0::downsample].T @ F2[:,0::downsample] # cos distance metric\n",
    "    \n",
    "    # run subseqDTW on chunks\n",
    "    seglen = int(np.ceil(F1.shape[1] / numSegments))\n",
    "    dn = steps[:,0].astype(np.uint32)\n",
    "    dm = steps[:,1].astype(np.uint32)\n",
    "    dw = weights\n",
    "    params1 = {'dn': dn, 'dm': dm, 'dw': dw, 'SubSequence': True}\n",
    "    Dparts = []\n",
    "    Bparts = []\n",
    "    for i in range(numSegments):\n",
    "        Cpart = C[i*seglen : min((i+1)*seglen, F1.shape[1]), :]\n",
    "        [D, B] = DTW_Cost_To_AccumCostAndSteps(Cpart, params1)\n",
    "        Dparts.append(D)\n",
    "        Bparts.append(B)\n",
    "    \n",
    "    # construct Cseg, Tseg\n",
    "    Cseg = np.zeros((numSegments, F2.shape[1]))\n",
    "    Tseg = np.zeros((numSegments, F2.shape[1]), dtype=np.int32)\n",
    "    for i, Dpart in enumerate(Dparts):\n",
    "        Cseg[i,:] = Dpart[-1,:]\n",
    "        Tseg[i,:] = calc_Tseg(Dpart, Bparts[i], params1)\n",
    "    \n",
    "    # segment-level DP\n",
    "    [Dseg, Bseg] = Segment_DP(Cseg, Tseg)\n",
    "    segmentEndIdxs = Segment_Backtrace(Dseg, Bseg)\n",
    "    \n",
    "    # backtrace on chunks\n",
    "    wps = []\n",
    "    for i, endidx in enumerate(segmentEndIdxs):\n",
    "        params2 = {'dn': dn, 'dm': dm, 'dw': dw, 'SubSequence': True, 'startCol': endidx}\n",
    "        [wpchunk, _, _] = DTW_GetPath(Dparts[i], Bparts[i], params2)\n",
    "        wpchunk[0,:] = wpchunk[0,:] + i*seglen  # account for relative offset\n",
    "        wps.append(wpchunk.copy())\n",
    "    wp_merged = np.hstack(wps)\n",
    "    \n",
    "    if swap:\n",
    "        wp_merged = np.flipud(wp_merged) # undo swap\n",
    "    \n",
    "    if outfile:\n",
    "        pickle.dump(wp_merged, open(outfile, 'wb'))\n",
    "        \n",
    "    return wp_merged#, segmentEndIdxs, seglen, Dparts, Bparts, params1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignParDTW_parallel(featfile1, featfile2, steps, weights, downsample, numSegments, outfile = None, C = None):\n",
    "    '''\n",
    "    Parallelized version of alignParDTW\n",
    "    '''\n",
    "    \n",
    "    # compute cost matrix\n",
    "    if C is None:\n",
    "        F1 = np.load(featfile1) # 12 x N\n",
    "        F2 = np.load(featfile2) # 12 x M\n",
    "        swap = (F1.shape[1] > F2.shape[1])\n",
    "        if swap:\n",
    "            F1, F2 = F2, F1 # make the shorter sequence the query\n",
    "        if max(F1.shape[1], F2.shape[1]) / min(F1.shape[1], F2.shape[1]) >= 2: # no valid path possible\n",
    "            if outfile:\n",
    "                pickle.dump(None, open(outfile, 'wb'))\n",
    "            return None\n",
    "        C = 1 - F1[:,0::downsample].T @ F2[:,0::downsample] # cos distance metric\n",
    "    \n",
    "    # prep data for parallelization\n",
    "    seglen = int(np.ceil(C.shape[0] / numSegments))\n",
    "    dn = steps[:,0].astype(np.uint32)\n",
    "    dm = steps[:,1].astype(np.uint32)\n",
    "    dw = weights\n",
    "    params1 = {'dn': dn, 'dm': dm, 'dw': dw, 'SubSequence': True}\n",
    "    inputs = []\n",
    "    for i in range(numSegments):\n",
    "        Cpart = C[i*seglen : min((i+1)*seglen, C.shape[0]), :]\n",
    "        inputs.append((Cpart, params1))\n",
    "\n",
    "    # run subseqDTW on chunks (parallelized)\n",
    "    pool = multiprocessing.Pool(numSegments)\n",
    "    outputs = pool.starmap(DTW_Cost_To_AccumCostAndSteps, inputs) # list of (D,B) tuples\n",
    "    pool.close() \n",
    "\n",
    "    # construct Cseg, Tseg\n",
    "    Cseg = np.zeros((numSegments, C.shape[1]))\n",
    "    Tseg = np.zeros((numSegments, C.shape[1]), dtype=np.int32)\n",
    "    for i, (Dpart, Bpart) in enumerate(outputs):\n",
    "        Cseg[i,:] = Dpart[-1,:]\n",
    "        Tseg[i,:] = calc_Tseg(Dpart, Bpart, params1)\n",
    "    \n",
    "    # segment-level DP\n",
    "    [Dseg, Bseg] = Segment_DP(Cseg, Tseg)\n",
    "    segmentEndIdxs = Segment_Backtrace(Dseg, Bseg)\n",
    "    \n",
    "    # backtrace on chunks\n",
    "    wps = []\n",
    "    for i, endidx in enumerate(segmentEndIdxs):\n",
    "        params2 = {'dn': dn, 'dm': dm, 'dw': dw, 'SubSequence': True, 'startCol': endidx}\n",
    "        Dpart, Bpart = outputs[i]\n",
    "        [wpchunk, _, _] = DTW_GetPath(Dpart, Bpart, params2)\n",
    "        wpchunk[0,:] = wpchunk[0,:] + i*seglen  # account for relative offset\n",
    "        wps.append(wpchunk.copy())\n",
    "    wp_merged = np.hstack(wps)\n",
    "    \n",
    "    if swap:\n",
    "        wp_merged = np.flipud(wp_merged) # undo swap\n",
    "    \n",
    "    if outfile:\n",
    "        pickle.dump(wp_merged, open(outfile, 'wb'))\n",
    "        \n",
    "    return wp_merged#, segmentEndIdxs, seglen, Dparts, Bparts, params1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featfile1 = 'features/clean/Chopin_Op017No4/Chopin_Op017No4_Afanassiev-2001_pid9130-01.npy'\n",
    "# featfile2 = 'features/clean/Chopin_Op017No4/Chopin_Op017No4_Ben-Or-1989_pid9161-12.npy'\n",
    "# #featfile1 = 'features/clean/Chopin_Op017No4/Chopin_Op017No4_Ashkenazy-1981_pid9058-13.npy'\n",
    "# #featfile1 = 'features/clean/Chopin_Op017No4/Chopin_Op017No4_Beliavsky-2004_pid9152-13.npy'\n",
    "# #featfile2 = 'features/clean/Chopin_Op017No4/Chopin_Op017No4_Biret-1990_pid9062-13.npy'\n",
    "# steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "# weights = np.array([1,1,2])\n",
    "# downsample = 1\n",
    "# numSegments = 5\n",
    "# #wp1 = alignDTW(featfile1, featfile2, steps, weights, downsample)\n",
    "# #wp2 = alignSegmentalDTW(featfile1, featfile2, steps, weights, downsample, numSegments)\n",
    "# wp3 = alignParDTW(featfile1, featfile2, steps, weights, downsample, numSegments)\n",
    "# wp4 = alignParDTW_parallel(featfile1, featfile2, steps, weights, downsample, numSegments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(wp1[0,:], wp1[1,:], 'k')\n",
    "# plt.plot(wp2[0,:], wp2[1,:], 'r')\n",
    "# plt.plot(wp3[0,:], wp3[1,:], 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_list = 'cfg_files/query.test.list'\n",
    "# featdir1 = Path('features/clean')\n",
    "# featdir2 = Path('features/clean')\n",
    "# n_cores = 2\n",
    "# steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "# weights = np.array([1,1,2])\n",
    "# downsample = 1\n",
    "# segmentVals = [2, 4, 8, 16, 32]\n",
    "# for numSegments in segmentVals:\n",
    "#     outdir = Path(f'experiments_test/pardtw_{numSegments}_clean')\n",
    "#     alignSegmentalDTW_batch(query_list, featdir1, featdir2, outdir, n_cores, steps, weights, downsample, numSegments, alignParDTW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure runtime of different DTW variants on cost matrices of varying sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running size = 1000 ..........Running size = 2000 ..........Running size = 5000 ..........Running size = 10000 ..........Running size = 20000 .........."
     ]
    }
   ],
   "source": [
    "# DTW\n",
    "outfile = 'dtw_prof.pkl'\n",
    "steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "weights = np.array([2,3,3])\n",
    "downsample = 1\n",
    "sizes = [1000, 2000, 5000, 10000, 20000, 50000]\n",
    "N = 10\n",
    "durs = np.zeros((len(sizes), N))\n",
    "for i in range(len(sizes)):\n",
    "    sz = sizes[i]\n",
    "    print(f'Running size = {sz} ', end='')\n",
    "    C = np.random.rand(sz, sz)\n",
    "    for j in range(N):\n",
    "        print('.', end='')\n",
    "        tStart = time.time()\n",
    "        alignDTW(None, None, steps, weights, downsample, C = C)\n",
    "        tEnd = time.time()\n",
    "        durs[i,j] = tEnd - tStart\n",
    "        gc.collect()\n",
    "    print('')\n",
    "pickle.dump([durs, sizes], open(outfile, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dtw_prof.pkl', 'rb') as f:\n",
    "    d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.76248312e-02, 8.12971592e-02, 4.66314220e-01, 1.84020312e+00,\n",
       "       7.30212922e+00, 4.88759911e+01])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(d[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running numSegments = 2 |.Running numSegments = 4 |.Running numSegments = 8 |.Running numSegments = 16 |.Running numSegments = 32 |."
     ]
    }
   ],
   "source": [
    "# SegmentalDTW\n",
    "outfile = 'segmental_prof.pkl'\n",
    "steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "weights = np.array([1,1,2])\n",
    "downsample = 1\n",
    "segmentVals = [2, 4, 8, 16, 32]\n",
    "#sizes = [1000, 2000, 5000, 10000, 20000, 50000]\n",
    "sizes = [20000]\n",
    "N = 1\n",
    "durs = np.zeros((len(segmentVals), len(sizes), N))\n",
    "for i, numSegments in enumerate(segmentVals):\n",
    "    print(f'Running numSegments = {numSegments} ', end='')\n",
    "    for j, sz in enumerate(sizes):\n",
    "        print('|', end='')\n",
    "        C = np.random.rand(sz, sz)\n",
    "        for k in range(N):\n",
    "            print('.', end='')\n",
    "            tStart = time.time()\n",
    "            alignSegmentalDTW(None, None, steps, weights, downsample, numSegments, C=C)\n",
    "            tEnd = time.time()\n",
    "            durs[i,j] = tEnd - tStart\n",
    "            gc.collect()\n",
    "#pickle.dump([durs, segmentVals, sizes], open(outfile, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.74982882],\n",
       "       [7.41562772],\n",
       "       [7.36769056],\n",
       "       [7.39883256],\n",
       "       [7.05567765]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(durs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParDTW\n",
    "outfile = 'pardtw_prof.pkl'\n",
    "steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "weights = np.array([1,1,2])\n",
    "downsample = 1\n",
    "segmentVals = [8]\n",
    "sizes = [1000, 2000, 5000, 10000, 20000, 50000]\n",
    "N = 10\n",
    "durs = np.zeros((len(segmentVals), len(sizes), N))\n",
    "for i, numSegments in enumerate(segmentVals):\n",
    "    print(f'Running numSegments = {numSegments} ', end='')\n",
    "    for j, sz in enumnerate(sizes):\n",
    "        print('|', end='')\n",
    "        C = np.random.rand(sz, sz)\n",
    "        for k in range(N):\n",
    "            print('.', end='')\n",
    "            tStart = time.time()\n",
    "            alignParDTW_parallel(None, None, steps, weights, downsample, numSegments, C=C)\n",
    "            tEnd = time.time()\n",
    "            durs[i,j] = tEnd - tStart\n",
    "            gc.collect()\n",
    "#pickle.dump([durs, segmentVals, sizes], open(outfile, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DropDTW",
   "language": "python",
   "name": "dropdtw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
